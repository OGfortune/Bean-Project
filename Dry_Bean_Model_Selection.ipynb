{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "bean_data = pd.read_csv('Dry_Bean_Dataset.csv')\n",
    "\n",
    "#first 5 rows \n",
    "bean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last 5 rows of data\n",
    "bean_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data dimension\n",
    "bean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying there are no constant features\n",
    "for i in bean_data.columns:\n",
    "    print(i,':', bean_data[i].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_data['Class'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an OrdinalEncoder\n",
    "\n",
    "ordinal_enc = OrdinalEncoder()\n",
    "\n",
    "# encoding Class feature, converting categories to numeral labels\n",
    "ordinal_labels = ordinal_enc.fit_transform(bean_data[['Class']])\n",
    "\n",
    "\n",
    "class_names = bean_data['Class']\n",
    "\n",
    "# Creating a dictionary to map class names to their ordinal labels\n",
    "target_dict = {class_name: encoding[0] for class_name, encoding in zip(class_names, ordinal_labels)}\n",
    "\n",
    "#Setting class encoded labels as class names\n",
    "bean_data['Class'] = ordinal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting target list\n",
    "target_dict = sorted(target_dict.items(), key=lambda x:x[1])\n",
    "target_list = [i[0] for i in target_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_data['Class'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting The dataset\n",
    "-----------------------------------\n",
    "In this section, I would be splitting the dataset into a test and train subsets. this method is also known as the hold out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting input (x) and target features (y)\n",
    "y= bean_data['Class']\n",
    "X= bean_data.drop(['Class'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardising data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataframe into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=30)\n",
    "print(\"original range is: \",bean_data.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour = KNeighborsClassifier(n_neighbors= 3, weights= 'distance')\n",
    "neighbour.fit(X_train, y_train)\n",
    "neighbour.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the trained model on unseen data (i.e. test)\n",
    "K_nearestNeigbor_test = neighbour.predict(X_test)\n",
    "\n",
    "print(\"==================== KNN on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, K_nearestNeigbor_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, K_nearestNeigbor_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, K_nearestNeigbor_test, target_names=[i for i in target_list]))\n",
    "print(\"=============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "----------------------------------------\n",
    "**1. Accuracy:** The overall accuracy of the KNN is 92%,indicating that abount 92% of the class labels were classified correctly by the model.\n",
    "\n",
    "**2. Classification Report:** \n",
    "\n",
    "* The BOMBAY class has a perfect precision, recall and F-score, with all three metrics having a score of 100%\n",
    "\n",
    "* The BARBUNYA class, has a precision of 95% and a recall of 91%\n",
    "\n",
    "* The DERMASON class has a precision of 91% and a recall 0f 92%\n",
    "\n",
    "* The HOROZ class, has a precision of 95% and a recall of 96%.\n",
    "\n",
    "* The SEKER class, has a precision of 95% and a recall of 96%\n",
    "\n",
    "* The SIRA class, has a precision and recall 86% \n",
    "\n",
    "##### Conclusion\n",
    "----------------------------\n",
    "\n",
    "The model seems to generalise well to unseen data, with an accuracy score of 92%. However, two classes in particular are of interest. The SIRA class (the only one where boyth recall and precision was below 90%) and the BOMBAY class which had a 100% precision and accuracy. These results could be due to the imbalanced nature of the class distribution, especially in relation to the BOMBAY class which is the least prevalent class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Tree_Test = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"==================== Decision Tree on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, Decision_Tree_Test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, Decision_Tree_Test))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, Decision_Tree_Test, target_names= [i for i in target_list]))\n",
    "print(\"=======================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Results\n",
    "----------------------------------------\n",
    "**1. Accuracy:** The overall accuracy of the Decision Tree is 89%,indicating that abount 89% of the class labels were classified correctly by the model.\n",
    "\n",
    "**2. Classification Report:** \n",
    "\n",
    "\n",
    "* The BARBUNYA class, has a precision, recall and  F1-score of 90%\n",
    "\n",
    "* The BOMBAY class has a perfect precision, recall and F-score, with all three metrics having a score of 100%\n",
    "\n",
    "* The CALI class has a precision of 93% and a recall 0f 92%\n",
    "\n",
    "* The DERMASON class has a precision of 89% and a recall 0f 90%\n",
    "\n",
    "* The HOROZ class, has a precision of 92% and a recall of 93%.\n",
    "\n",
    "* The SEKER class, has a precision of 92% and a recall of 92%\n",
    "\n",
    "* The SIRA class, has a precision and recall 82% \n",
    "\n",
    "The macro average and weighted average across all 3 evaluation metrics (i.e. precision, recall and F1_Score) are 91% and 89% respectively \n",
    "\n",
    "##### Conclusion\n",
    "---------------------------- \n",
    "The decision tree performs really well on the test (or unseen) data, with an accuracy of 89%. The precision and recall per class was over 90% except in the SIRA and DERMASON classes. The models ability in predicting the SIRA class was the lowest, with precision and recall of 82%. Again, this results might be attributable to the skewed nature of the data distribution and also number of available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regression = LogisticRegression(solver='sag', max_iter=1000)\n",
    "log_regression.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logisticreg_predictions_test = log_regression.predict(X_test)\n",
    "\n",
    "print(\"==================== Logistic Regression on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test, target_names= [i for i in target_list]))\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Results\n",
    "----------------------------------------\n",
    "**1. Accuracy:** The overall accuracy of the Logistic regression is 93%,indicating that abount 93%% of the class labels were classified correctly by the model.\n",
    "\n",
    "**2. Classification Report:** \n",
    "\n",
    "\n",
    "* The BARBUNYA class, has a precision of 95%, recall of 90% and  F1-score of 92%\n",
    "\n",
    "* The BOMBAY class has a perfect precision, recall and F-score, with all three metrics having a score of 100%\n",
    "\n",
    "* The CALI class has a precision of 92% and a recall 0f 94%\n",
    "\n",
    "* The DERMASON class has a precision of 92% and a recall 0f 92%\n",
    "\n",
    "* The HOROZ class, has a precision of 95% and a recall of 97%.\n",
    "\n",
    "* The SEKER class, has a precision of 95% and a recall of 95%\n",
    "\n",
    "* The SIRA class, has a precision and recall 87% \n",
    "\n",
    "The macro average 94% across all 3 metrics, and weighted average for precision, recall and F1_Score are 93%, 93%, 92% respectively \n",
    "\n",
    "##### Conclusion\n",
    "---------------------------- \n",
    "The logisic regression has an overall accuracy of 93%. The model performed best at predicting the Bombay class, with a precision and recall score of 100%.\n",
    "For the other classes, the model had a precision and recall of 90% and above, except in the SIRA class which was the most misclassified, with a precision and accuracy of 87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_prediction_test = random_forest.predict(X_test)\n",
    "\n",
    "print(\"==================== Random Forest on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, random_forest_prediction_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, random_forest_prediction_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, random_forest_prediction_test, target_names= [i for i in target_list]))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Results\n",
    "----------------------------------------\n",
    "**1. Accuracy:** The overall accuracy of the Random forest is 93%,indicating that abount 93%% of the class labels were classified correctly by the model.\n",
    "\n",
    "**2. Classification Report:** \n",
    "\n",
    "\n",
    "* The BARBUNYA class, has a precision of 96%, recall of 91% and  F1-score of 92%\n",
    "\n",
    "* The BOMBAY class has a perfect precision, recall and F-score, with all three metrics having a score of 100%\n",
    "\n",
    "* The CALI class has a precision of 93% and a recall 0f 94%\n",
    "\n",
    "* The DERMASON class has a precision of 90% and a recall 0f 94%\n",
    "\n",
    "* The HOROZ class, has a precision of 96% and a recall of 96%.\n",
    "\n",
    "* The SEKER class, has a precision of 95% and a recall of 95%\n",
    "\n",
    "* The SIRA class, has a precision 89% and recall 85% \n",
    "\n",
    "The macro average and weighted average across all 3 evaluation metrics (i.e. precision, recall and F1_Score) are 94% and 93% respectively \n",
    "\n",
    "##### Conclusion\n",
    "---------------------------- \n",
    "The randomforest classifier has an accuracy of 94%, and performed really well in predicting or assingning the correct class label for the unseen data. The precision and recall for each class exceeded over 85%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN Accuracy:\", round(accuracy_score(y_test, K_nearestNeigbor_test),2))\n",
    "print(\"Decision Tree Accuracy:\", round(accuracy_score(y_test, Decision_Tree_Test),2))\n",
    "print(\"Logistic Regression Accuracy:\", round(accuracy_score(y_test, multiple_logisticreg_predictions_test),2))\n",
    "print(\"Random Forest Accuracy:\", round(accuracy_score(y_test, random_forest_prediction_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Results\n",
    "---------------\n",
    "The logistic Regression and the Random Forest models were the best performing, with both having an accuracy of 93%. This suggest this two model in particular, perfoem well in correctly classifying unseen data to the correct class. The K nearest neighbor was the second best model with an accuracy of 92%, while the Decision tree was the least perfoming model with an accuracy of 89%. The weighted average precision, recall and f1-score was also over 90% in all the models excepty the decision tree model where they were 89%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SIRA CLASSIFICATION\n",
    "\n",
    "In our data, one of the classes, the SIRA class, is moderately poisonous and the risk of misclassification potentially having adverse effect on the health of the consumer. Therefore, to avoid this potential hazardous effect, we would be more focused on the evaluation metric, Recall. Recall is a measure of the sensitivity or the ability of the model to correctly predict a class when it is positive (i.e. true positive). This is better as metric because it tells us the likelihood of members of this class the SIRA class being missed.\n",
    "\n",
    "Most machine learning models assume distribution between classes is balanced by default, hence they tend to be bias to the majority class. To improve each model's ability to correctly find true positives of the SIRA class I would be making adjustments at the machine learning model level and data level and comparing results.\n",
    "\n",
    "1. **Machine Learning Level:** One way to enhance the SIRA class is by specifying class weights at the model level. This essentially places an importance on a particular class over the others, effectively imposing a cost or penalty for misclassifying the important class or label. By informing the model which class is more important,  it places more emphasis on accurately predicting the specified class.  \n",
    "\n",
    "2. **Data Level:** Another way of achieving this is to either oversample or undersample the class labels. Undersampling typically results in significant loss in data which is not ideal, therefore over samppling is often prefered, but this come at the risk of overfitting. To curb the issue of overfiting, I would be using Synthetic Minority Over-sampling Technique (SMOTE) technique, which generates synthetic instances of the minority class by interpolating between instances of the minority class.  The effective usage of the SMOTE technique should balance our dataset and should theoretically remove the bias of all our models to the majority class, improving its ability in correctly predicting minority classes when they are positve.\n",
    "\n",
    "In this section, i would be attempting to improve the each model's recall metric for the SIRA class in two ways. \n",
    "\n",
    "a. using class weights\n",
    "\n",
    "b. using oversampling(SMOTE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Class Weights\n",
    "\n",
    "**Please Note:** nearest neighbor doesn;y have a class weight option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning class weight of the SIRA class\n",
    "sira_weight={6.0: 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regression = LogisticRegression(max_iter=1000, class_weight=sira_weight)\n",
    "log_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logisticreg_predictions_test = log_regression.predict(X_test)\n",
    "\n",
    "print(\"==================== Class Weighted Logistic Regression on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test, target_names= [i for i in target_list]))\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "-------------------------------\n",
    "The class weight significantly improved the logistics regression's ability in accurately classifying the SIRA class, wth a recall of 97% vs 87% in the model without class weights. However, the precision score went down from 87% to 73% and the overall accuracy of the model on the entire model also declined from 93% to 91%. The changes in the precision vs recall score is potentially due to the inverse relatonship between both, since our model is now more concerned with correctly predicting positve instances (i.e. true positive) of the SIRA class. Since we are mainly concerned with the potential harzardous effect of the SIRA class this could be considered a good trade-off, especially considering there were only minor impacts on the other classes and the F1-score between the original logistic model and the class weighted one defers by 3% 87% vs 84% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Random Forest with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_prediction_test = random_forest.predict(X_test)\n",
    "\n",
    "print(\"==================== Class Weighted Random Forest on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, random_forest_prediction_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, random_forest_prediction_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, random_forest_prediction_test, target_names= [i for i in target_list]))\n",
    "print(\"======================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result\n",
    "----------------\n",
    "\n",
    "Assigning class weights to the SIRA class did not significantly affect the random forest model. The model's accuracy remained constant at 93%, and only a 1% increase in the model's ability to correctly predict the SIRA class (86% class weighted model vs 85% original model). The precision score of the SIRA class also had a decrease of 1%, a nod towards the sometime inverse relationship between precision and recall. reasons the random forest model might not have had as much impact on the SIRA class even with the implementation of class weights are expressed below:\n",
    "\n",
    "- the implementation of class weights on random forest is different from logistic regression where misclassification is penalised\n",
    "\n",
    "- random forest models are very robust even when handling imbalanced data, because their results are a result of the aggregation of many decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree with class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(class_weight=sira_weight, criterion='entropy')\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Tree_Test = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"==================== Class Weighted Decision Tree on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, Decision_Tree_Test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, Decision_Tree_Test))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, Decision_Tree_Test, target_names= [i for i in target_list]))\n",
    "print(\"======================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "----------------------------------------------\n",
    "The decision tree overall accuracy remained constant in comparison to the original model, with both having an accuracy of 90%. However, the clss weighted model performed better at correctly classifying actual instances of the SIRA class with a recall 84% vs 81% in the original decision tree model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall Results for Class Weighted Model\n",
    "-------------------------------------------------\n",
    "\n",
    "All three models recorded improvements in their ability to correctly classify instances of the SIRA class. By far the model with the biggest improvements was the Logistic regression which saw a 10% increase in recall from 87% to 97% in the class weighted model. This is potentially due to the implementation of class weight in logistic regression models, as they act as penalty scores for misinterpretation, in comparison to their implementation in decision trees and random forest where they influence node impurity.. On the otherhand, random forest and decision tree models had varying levels of improvements. The random forest model only saw aa recall increase of 1% while the decision tree model increased by 3% for the SIRA class.\n",
    "\n",
    "Overall, we can conclude that assigning class weights resulted in significant improvements in classifying the SIRA class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_smote, y_smote = SMOTE(random_state=30).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=3, weights='distance').fit(x_smote, y_smote)\n",
    "k_neigbor = KNN.predict(X_test)\n",
    "\n",
    "print(\"==================== SMOTE adjusted K-Nearest Neighbour on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, k_neigbor))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, k_neigbor))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, k_neigbor, target_names= [i for i in target_list]))\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SMOTE alone to balance the data distribution did not seem to have much impact on the K nearest neighbour model's ability to classify the SIRA class. In both cases (i.e. original model vs SMOTE model), the recall metric score was 86%, however, in the SMOTE model, the precision metric score for the SIRA class was 85%, a reduction of 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(criterion='entropy').fit(x_smote, y_smote)\n",
    "Decision_Tree_Test = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"==================== SMOTE adjusted decision tree on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, Decision_Tree_Test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, Decision_Tree_Test))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, Decision_Tree_Test, target_names= [i for i in target_list]))\n",
    "print(\"======================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result\n",
    "----------------------\n",
    "Using smote had a significant impact on the decicion's tree ability to correctly predict the SIRA class, with the recall metric score increasing from 80% in the original model to 84% in the SMOTE model. The overall accuracy of the model on the data also had a slight increase from 89% to 90%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000).fit(x_smote, y_smote)\n",
    "multiple_logisticreg_predictions_test = log_reg.predict(X_test)\n",
    "\n",
    "print(\"==================== SMOTE adjusted Logistic Regression on Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test, target_names= [i for i in target_list]))\n",
    "print(\"=============================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result\n",
    "---------------\n",
    "Using SMOTE did not seem to have any significant effect on the logistic regression model's ability to predict the SIRA class.  There was a slight increase in the recall from 87% in the original model vs 88% in the SMOTE model. Overall we only observed a small change between both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forrest with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier().fit(x_smote, y_smote)\n",
    "\n",
    "random_forest_prediction_test = random_forest.predict(X_test)\n",
    "\n",
    "print(\"==================== SMOTE adjusted Random Forest Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, random_forest_prediction_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, random_forest_prediction_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, random_forest_prediction_test, target_names= [i for i in target_list]))\n",
    "print(\"=====================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "----------------------\n",
    "Balancing the data using SMOTE did not seem to impact the random forrest's abilility to correctly classify the SIRA class by much. The recall metric score for the SIRA class increased by only 3% from 85% in the original model to 88% in the SMOTE model, while the precision for this class reduced by 1% from 90% in the original to 89% in the SMOTE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Results\n",
    "-------------------------------\n",
    "Balancing the data using SMOTE had varied effect across the various models tested. In some models like the Knearest neighbor, it had no impact on the models recall ability for the SIRA class, while in the logisitc regression, the SIRA class recall increased by 1% from 87% in the original vs 88% in the SMOTE model. Both the decision tree, and the random forest models saw thei SIRA recall score improved by 3%, the biggest changes in the SMOTE trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Model name      | Accuracy | SIRA recall |\n",
    "| --------------- | -------- | ----------- |\n",
    "| Log regression (original)      | 93%   | 87%     |\n",
    "|Logistic regression (class_weights)|91% | 97%  |\n",
    "|Logistic regression (SMOTE)|92% | 88%  |\n",
    "| Decision Tree (original) | 89%    | 81%      |\n",
    "| Decision Tree (class_weights) | 90%     | 84%       |\n",
    "| Decision Tree (SMOTE) | 90%     | 84%       |\n",
    "| Random Forest (original)  | 93%    | 85%    |\n",
    "| Random Forest (class_weights)  | 93%     | 86%        |\n",
    "| Random Forest (SMOTE)  | 93%     | 88%        |\n",
    "|K nearest-neighbour (original) | 92% | 86%  |\n",
    "|K nearest-neighbour (SMOTE) | 91% | 86%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilising SMOTE to balance data distribution or using class weights to assign cost of misclassification, saw improvements in the models's ability to correctly classify actual instances of the SIRA class. By far the best performing model was the Logistic regression model, with a recall score of 97% for the sira class, significantly reducing the risk of the SIRA bean being misclassified to just 3%. in the SMOTE balanced data, the logistic model's recall was also on par with the random model with both having recall score of 88%. \n",
    "\n",
    "In conclusion, while SMOTE balanced model saw enhancements in their ability to correctly identify SIRA beans, they were outperformed by the class weighted models which placed a penalty or cost on misclassifying the SIRA class. The approach of assigning weights to the class seem to have a bigger impact on the models ability to correctly classify this class than just balancing the data. It offers a valuable technique for addressing the challenges posed by imbalanced datasets and enhancing the safety of the SIRA class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Note:** I tried combining SMOTE and class_weight but this had no impact any of the models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
